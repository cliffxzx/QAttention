{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path += ['../']\n",
    "from utils.lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Model for Quick POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 06:15:40.830794: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 06:15:40.838135: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRFinder: Finding `best_base_lr` range from: [5e-06(0.00000500), 1e-02(0.01000000)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LRFinder: logging loss:   0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/projects/QNet/experiments/LRFinder.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdgx/home/jovyan/projects/QNet/experiments/LRFinder.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m opt \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlr_decayed_fn, beta_1\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, beta_2\u001b[39m=\u001b[39m\u001b[39m0.98\u001b[39m, epsilon\u001b[39m=\u001b[39m\u001b[39m1e-09\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdgx/home/jovyan/projects/QNet/experiments/LRFinder.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mopt, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdgx/home/jovyan/projects/QNet/experiments/LRFinder.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_data, validation_data\u001b[39m=\u001b[39;49mtest_data, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/QNet-80tnyBMQ/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/projects/QNet/utils/lr_finder.py:107\u001b[0m, in \u001b[0;36mLRFinder.on_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m lr_decayed_fn\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmss_l \u001b[39m=\u001b[39m MaxStepStoppingWithLogging(max_steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps, tqdm_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLRFinder\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave_weights(\u001b[39m'\u001b[39;49m\u001b[39mmodel.tf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata,\n\u001b[1;32m    110\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     callbacks\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmss_l]\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_weights(\u001b[39m'\u001b[39m\u001b[39mmodel.tf\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path += ['../']\n",
    "from utils.lr_finder import LRFinder\n",
    "\n",
    "np.random.seed(43)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "epoch = 5\n",
    "batch_size = 32\n",
    "sample_size = 50\n",
    "train_data = tf.data.Dataset.from_tensor_slices((np.zeros((sample_size, batch_size, 10, 3)),np.random.rand(sample_size, batch_size, 1)))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((np.zeros((sample_size, batch_size, 10, 3)), np.random.rand(sample_size, batch_size, 1)))\n",
    "\n",
    "callbacks = [LRFinder(train_data, batch_size, window_size=4, max_steps=400, filename='logs/lr_finder')]\n",
    "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay((4,400), epoch * len(train_data), alpha=1e-2)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_decayed_fn, beta_1=0.9, beta_2=0.98, epsilon=1e-09)\n",
    "model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "model.fit(train_data, validation_data=test_data, batch_size=batch_size, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Model for Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Version:  2.7.0\n",
      "Eager mode:  True\n",
      "GPU is NOT AVAILABLE\n",
      "Num Replicas In Sync:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 18:06:52.020167: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 18:07:59.295955: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " f_net_1 (FNet)              (None, 8, 16)             76720     \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 8, 7)             119       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,839\n",
      "Trainable params: 76,839\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1851/1851 [==============================] - 63s 32ms/step - loss: 0.3685 - accuracy: 0.8973 - val_loss: 0.3103 - val_accuracy: 0.9055 - val_f1_score: 0.0129\n",
      "Epoch 2/5\n",
      "1851/1851 [==============================] - 57s 29ms/step - loss: 0.2926 - accuracy: 0.9102 - val_loss: 0.2858 - val_accuracy: 0.9118 - val_f1_score: 0.0133\n",
      "Epoch 3/5\n",
      "1851/1851 [==============================] - 55s 29ms/step - loss: 0.2714 - accuracy: 0.9149 - val_loss: 0.2735 - val_accuracy: 0.9146 - val_f1_score: 0.0141\n",
      "Epoch 4/5\n",
      "1851/1851 [==============================] - 61s 31ms/step - loss: 0.2572 - accuracy: 0.9183 - val_loss: 0.2657 - val_accuracy: 0.9163 - val_f1_score: 0.0150\n",
      "Epoch 5/5\n",
      "1851/1851 [==============================] - 53s 27ms/step - loss: 0.2476 - accuracy: 0.9207 - val_loss: 0.2637 - val_accuracy: 0.9168 - val_f1_score: 0.0148\n",
      "Log file saved at:  logs/qnet-1668190375.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path += ['../']\n",
    "from utils.lr_finder import LRFinder\n",
    "\n",
    "import os, json, time\n",
    "from datasets import get_dataset\n",
    "from trainers import get_trainer\n",
    "\n",
    "def save_log(history, val_metric: str=None):\n",
    "    logs = { 'history': history }\n",
    "\n",
    "    if val_metric != None:\n",
    "        logs['best_acc'] = max(history[val_metric])\n",
    "        print('Best score: ', logs['best_acc'])\n",
    "\n",
    "    logfile_name = f'logs/qnet-{int(time.time())}.json'\n",
    "    os.makedirs(os.path.dirname(logfile_name), exist_ok=True)\n",
    "    with open(logfile_name, 'w') as f:\n",
    "        json.dump(logs, f, indent=4)\n",
    "    print('Log file saved at: ', logfile_name)\n",
    "\n",
    "class Args:\n",
    "    lr = 3e-4 * 128\n",
    "    batch_size = 128\n",
    "    seq_len = 8\n",
    "    epochs = 5\n",
    "    model = 'fnet'\n",
    "    embed_size = 16\n",
    "    num_blocks = 1\n",
    "    qnet_depth = 1\n",
    "    lr_finder = [4,400,'logs/lr_finder']\n",
    "    dataset = 'msra'\n",
    "    # dataset = 'colbert'\n",
    "    # dataset = 'rentrunway'\n",
    "    # dataset = 'msra'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n",
    "\n",
    "dataset = get_dataset(args.dataset)\n",
    "trainer = get_trainer(dataset.getTask())\n",
    "\n",
    "fitting = trainer.train(args, dataset)\n",
    "\n",
    "if dataset.getTask() == 'classification':\n",
    "    if dataset.getOutputSize() > 2:\n",
    "        save_log(fitting.history, 'val_categorical_accuracy')\n",
    "    else:\n",
    "        save_log(fitting.history, 'val_binary_accuracy')\n",
    "else:\n",
    "    save_log(fitting.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('QNet-80tnyBMQ')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "194ce798efaa7768c776effb78dc6a27ac2f4434f28b58117e0469f54c5184ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
