@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@misc{devlin2018bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year={2018},
    eprint={1810.04805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{
    dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@misc{arxiv.2006.04768,
  doi = {10.48550/ARXIV.2006.04768},
  url = {https://arxiv.org/abs/2006.04768},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Linformer: Self-Attention with Linear Complexity},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@article{melas2021you,
  title={Do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet},
  author={Melas-Kyriazi, Luke},
  journal={arXiv preprint arXiv:2105.02723},
  year={2021}
}


@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 volume = {33},
 year = {2020}
}

@article{DBLP:journals/corr/abs-2105-02723,
  author    = {Luke Melas{-}Kyriazi},
  title     = {Do You Even Need Attention? {A} Stack of Feed-Forward Layers Does
               Surprisingly Well on ImageNet},
  journal   = {CoRR},
  volume    = {abs/2105.02723},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.02723},
  eprinttype = {arXiv},
  eprint    = {2105.02723},
  timestamp = {Wed, 12 May 2021 15:54:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-02723.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2105-03824,
  author    = {James Lee{-}Thorp and
               Joshua Ainslie and
               Ilya Eckstein and
               Santiago Onta{\~{n}}{\'{o}}n},
  title     = {FNet: Mixing Tokens with Fourier Transforms},
  journal   = {CoRR},
  volume    = {abs/2105.03824},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.03824},
  eprinttype = {arXiv},
  eprint    = {2105.03824},
  timestamp = {Fri, 14 May 2021 12:13:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-03824.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2021_4cc05b35,
 author = {Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {9204--9215},
 publisher = {Curran Associates, Inc.},
 title = {Pay Attention to MLPs},
 url = {https://proceedings.neurips.cc/paper/2021/file/4cc05b35c2f937c5bd9e7d41d3686fff-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{chow2021ibm,
  title={IBM Quantum breaks the 100-qubit processor barrier},
  author={Chow, Jerry and Dial, Oliver and Gambetta, Jay},
  journal={IBM Research Blog},
  year={2021}
}

@article{huang2022quantum,
  title={Quantum advantage in learning from experiments},
  author={Huang, Hsin-Yuan and Broughton, Michael and Cotler, Jordan and Chen, Sitan and Li, Jerry and Mohseni, Masoud and Neven, Hartmut and Babbush, Ryan and Kueng, Richard and Preskill, John and others},
  journal={Science},
  volume={376},
  number={6598},
  pages={1182--1186},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{schuld2022quantum,
  title={Is quantum advantage the right goal for quantum machine learning?},
  author={Schuld, Maria and Killoran, Nathan},
  journal={arXiv preprint arXiv:2203.01340},
  year={2022}
}

@article{schuld2014quest,
  title={The quest for a quantum neural network},
  author={Schuld, Maria and Sinayskiy, Ilya and Petruccione, Francesco},
  journal={Quantum Information Processing},
  volume={13},
  number={11},
  pages={2567--2586},
  year={2014},
  publisher={Springer}
}

@article{wan2017quantum,
  title={Quantum generalisation of feedforward neural networks},
  author={Wan, Kwok Ho and Dahlsten, Oscar and Kristj{\'a}nsson, Hl{\'e}r and Gardner, Robert and Kim, MS},
  journal={npj Quantum information},
  volume={3},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{farhi2018classification,
  title={Classification with quantum neural networks on near term processors},
  author={Farhi, Edward and Neven, Hartmut},
  journal={arXiv preprint arXiv:1802.06002},
  year={2018}
}

@article{mitarai2018quantum,
  title={Quantum circuit learning},
  author={Mitarai, Kosuke and Negoro, Makoto and Kitagawa, Masahiro and Fujii, Keisuke},
  journal={Physical Review A},
  volume={98},
  number={3},
  pages={032309},
  year={2018},
  publisher={APS}
}

@article{beer2020training,
  title={Training deep quantum neural networks},
  author={Beer, Kerstin and Bondarenko, Dmytro and Farrelly, Terry and Osborne, Tobias J and Salzmann, Robert and Scheiermann, Daniel and Wolf, Ramona},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--6},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{naseem2020transformer,
  title={Transformer based deep intelligent contextual embedding for twitter sentiment analysis},
  author={Naseem, Usman and Razzak, Imran and Musial, Katarzyna and Imran, Muhammad},
  journal={Future Generation Computer Systems},
  volume={113},
  pages={58--69},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{wang2020transmodality,
  title={Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis},
  author={Wang, Zilong and Wan, Zhaohong and Wan, Xiaojun},
  booktitle={Proceedings of The Web Conference 2020},
  pages={2514--2520},
  year={2020}
}

@article{vaswani2018tensor2tensor,
  title={Tensor2tensor for neural machine translation},
  author={Vaswani, Ashish and Bengio, Samy and Brevdo, Eugene and Chollet, Francois and Gomez, Aidan N and Gouws, Stephan and Jones, Llion and Kaiser, {\L}ukasz and Kalchbrenner, Nal and Parmar, Niki and others},
  journal={arXiv preprint arXiv:1803.07416},
  year={2018}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@inproceedings{dong2018speech,
  title={Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition},
  author={Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5884--5888},
  year={2018},
  organization={IEEE}
}

@inproceedings{zandie2020emptransfo,
  title={Emptransfo: A multi-head transformer architecture for creating empathetic dialog systems},
  author={Zandie, Rohola and Mahoor, Mohammad H},
  booktitle={The Thirty-Third International Flairs Conference},
  year={2020}
}

@article{suglia2021embodied,
  title={Embodied bert: A transformer model for embodied, language-guided visual task completion},
  author={Suglia, Alessandro and Gao, Qiaozi and Thomason, Jesse and Thattai, Govind and Sukhatme, Gaurav},
  journal={arXiv preprint arXiv:2108.04927},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{PhysRevA.81.032324,
  title = {Optimal quantum learning of a unitary transformation},
  author = {Bisio, Alessandro and Chiribella, Giulio and D'Ariano, Giacomo Mauro and Facchini, Stefano and Perinotti, Paolo},
  journal = {Phys. Rev. A},
  volume = {81},
  issue = {3},
  pages = {032324},
  numpages = {6},
  year = {2010},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.81.032324},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.81.032324}
}

@article{PhysRevLett.122.170502,
  title = {Optimal Probabilistic Storage and Retrieval of Unitary Channels},
  author = {Sedl\'ak, Michal and Bisio, Alessandro and Ziman, M\'ario},
  journal = {Phys. Rev. Lett.},
  volume = {122},
  issue = {17},
  pages = {170502},
  numpages = {6},
  year = {2019},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.122.170502},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.122.170502}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@INPROCEEDINGS{892140,
    author={Cleve, R. and Watrous, J.}, 
    booktitle={Proceedings 41st Annual Symposium on Foundations of Computer Science},
    title={Fast parallel circuits for the quantum Fourier transform},
    year={2000},
    volume={},
    number={},
    pages={526-536},
    doi={10.1109/SFCS.2000.892140}
}

@inproceedings{NEURIPS2020_0ec96be3,
 author = {Bausch, Johannes},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1368--1379},
 publisher = {Curran Associates, Inc.},
 title = {Recurrent Quantum Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/0ec96be397dd6d3cf2fecb4a2d627c1c-Paper.pdf},
 volume = {33},
 year = {2020}
}

@INPROCEEDINGS{9747369,
  author={Chen, Samuel Yen-Chi and Yoo, Shinjae and Fang, Yao-Lung L.},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Quantum Long Short-Term Memory}, 
  year={2022},
  volume={},
  number={},
  pages={8622-8626},
  doi={10.1109/ICASSP43922.2022.9747369}
}


@inproceedings{abadi2016tensorflow,
  title={$\{$TensorFlow$\}$: a system for $\{$Large-Scale$\}$ machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

@article{broughton2020tensorflow,
  title={Tensorflow quantum: A software framework for quantum machine learning},
  author={Broughton, Michael and Verdon, Guillaume and McCourt, Trevor and Martinez, Antonio J and Yoo, Jae Hyeon and Isakov, Sergei V and Massey, Philip and Halavati, Ramin and Niu, Murphy Yuezhen and Zlokapa, Alexander and others},
  journal={arXiv preprint arXiv:2003.02989},
  year={2020}
}
@data{fw8e-z983-21,
    doi = {10.21227/fw8e-z983},
    url = {https://dx.doi.org/10.21227/fw8e-z983},
    author = {Annamoradnejad, Issa},
    publisher = {IEEE Dataport},
    title = {ColBERT dataset - 200k short texts for humor detection},
    year = {2021}
}

@inproceedings{xu-etal-2015-short,
    title = "Short Text Clustering via Convolutional Neural Networks",
    author = "Xu, Jiaming  and
      Wang, Peng  and
      Tian, Guanhua  and
      Xu, Bo  and
      Zhao, Jun  and
      Wang, Fangyuan  and
      Hao, Hongwei",
    booktitle = "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-1509",
    doi = "10.3115/v1/W15-1509",
    pages = "62--69",
}

@inproceedings{10.1145/3240323.3240398,
    author = {Misra, Rishabh and Wan, Mengting and McAuley, Julian},
    title = {Decomposing Fit Semantics for Product Size Recommendation in Metric Spaces},
    year = {2018},
    isbn = {9781450359016},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3240323.3240398},
    doi = {10.1145/3240323.3240398},
    abstract = {Product size recommendation and fit prediction are critical in order to improve customers' shopping experiences and to reduce product return rates. Modeling customers' fit feedback is challenging due to its subtle semantics, arising from the subjective evaluation of products, and imbalanced label distribution. In this paper, we propose a new predictive framework to tackle the product fit problem, which captures the semantics behind customers' fit feedback, and employs a metric learning technique to resolve label imbalance issues. We also contribute two public datasets collected from online clothing retailers.},
    booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
    pages = {422–426},
    numpages = {5},
    location = {Vancouver, British Columbia, Canada},
    series = {RecSys '18}
}

@article{miga2020telomere,
  title={Telomere-to-telomere assembly of a complete human X chromosome},
  author={Miga, Karen H and Koren, Sergey and Rhie, Arang and Vollger, Mitchell R and Gershman, Ariel and Bzikadze, Andrey and Brooks, Shelise and Howe, Edmund and Porubsky, David and Logsdon, Glennis A and others},
  journal={Nature},
  volume={585},
  number={7823},
  pages={79--84},
  year={2020},
  publisher={Nature Publishing Group}
}
