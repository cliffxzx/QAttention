\section{Introduction}

% Introduce quantum machine learning
Utilizing quantum computer on machine learning is a challenging task. Other than the gradient vanishing or exploding problem that already exists in classical computer, in the NISQ devices the qubits will incur decoherence after a long period of time. In addition, by the time that quantum computer is still developing, the quantum computing resource is limited. Researchers have to propose quantum machine learning model that can be verified on the current stage of NISQ and extensible for the large scale NISQ devices in the future.

% Introduce Transformer
Among various machine learning algorithms, the Transformer architecture family has achieved rapid and widespread dominance in various tasks, especially the models that use Transformer Encoder as its main component, for example, BERT-based model and Vision Transformer. Each block in the Transformer Encoder is composed of self Multi-head self-attention and Feedforward layer. The former is an inductive bias that connects each token in the input through a relevance weighted basis of every other token, and the latter is a linear transformation on each token.

% Introduce the problem -> large training cost (memory and time)
The one crucial shortcoming of Transformer is the time complexity of Multi-head attention. It performs matrix multiplication operation that needs time complexity of $O(n^2 \cdot d)$ to relate the tokens. This defect makes the time cost of directly input the long sequence to the model unacceptable. Multiple works~\cite{} have attempted to improve the performance of the Transformer Encoder, and most of them need to exchange accuracy with the speed.

% Introduce the potential of quantum computer
On the other hand, the hardware technologies of quantum computer are getting mature, which makes quantum algorithm as a possible solution to reduce the training cost of neural networks. At the time this paper is written, the quantum computer with 127 qubits was revealed~\cite{}. However, the actual capability of Quantum Machine Learning (QML) is still unknown. Multiple reports~\cite{} show that it indeed outperforms classical machine learning in certain situations. Most of the implementations of QML built on top of Variational Quantum Circuits (VQC), a promising class of algorithms that emerged as a central sub-field of QML. Based on VQC, multiple quantum Feedforward neural networks capable of universal quantum computation have been proposed~\cite{}, with the goal of using the potential of quantum computers in machine learning and attempting to train networks more effectively.

% Introduce the work.
In this work, we propose QNet, a novel sequence encoder model that entirely inferences on quantum computer. The architecture of the model is inspired by Transformer Encoder. The QNet reduces the inference time complexity while showing improvement on several tasks compared to other models.

% Show contribution, on XXXX datasets, the comparison
Two datasets, StackOverflow dataset and ColBERT dataset, are used to evaluation in the work. In the Stackoverflow dataset, QNet achieve accuracy 58.47 when embedding size is 2, while Transformer and FNet only reach 7.15 and 5.56 respectively. In the ColBERT dataset, with only 1 embedding dimension QNet can achieve accuracy 91.58 others need to have embedding dimension much larger than 1 to get to higher than 90. Therefore, we shows the quantum advantage on natural language processing tasks by using extremely small embedding dimension. 

% Show paper organization
This paper is organized to 7 sections. The introduction section points out the purpose of the work. The related work section shows the existing works that is related. The preliminary section brings in the required quantum computing knowledge. The method section describe the purposed model and the details of the work. The analysis section examine the time complexity of the model and compared with others. The experiments section verify the practicability of the work. In the last section, we conclude our work.

