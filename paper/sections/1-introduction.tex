\section{Introduction}

% Introduce quantum machine learning
Utilizing quantum computer on machine learning is a challenging task. Other than the gradient vanishing or exploding problem that already exists in classical computer, in the NISQ devices the qubits will incur decoherence after a long period of execution. In addition, by the time that quantum computer is still developing, the quantum computing resource is limited. Quantum machine learning researchers have to propose model that can be verified on the current stage of NISQ devices and extensible for the large scale NISQ devices in the future.

% Introduce Transformer
Among various machine learning algorithms, the Transformer~\cite{NIPS2017_3f5ee243} architecture family has achieved rapid and widespread dominance in various tasks, especially the models that use Transformer Encoder as its main component, for example, BERT~\cite{devlin2018bert} model and Vision Transformer~\cite{dosovitskiy2021an}. Each block in the Transformer Encoder is composed of Multi-head self-attention and Feedforward layer. The former is an inductive bias that connects each token in the input through a relevance weighted basis of every other token, and the latter is a parameterized linear transformation on each token.

% Introduce the problem -> large training cost (memory and time)
The one crucial shortcoming of Transformer is the time complexity of Multi-head self-attention. It performs matrix multiplication operation that needs time complexity of $O(n^2 \cdot d)$ to relate the tokens. This defect makes the time cost of directly input the long sequence to the model unacceptable. Multiple works~\cite{arxiv.2006.04768}~\cite{tolstikhin2021mlp}~\cite{melas2021you}~\cite{DBLP:journals/corr/abs-2105-03824}~\cite{NEURIPS2021_4cc05b35} have attempted to improve the performance of the Transformer Encoder, and most of them need to exchange accuracy with the speed.

% Introduce the potential of quantum computer
On the other hand, the hardware technologies of quantum computer are getting mature, which makes quantum algorithm as a possible solution to reduce the training cost of neural networks. At the time this paper is written, the quantum computer with 127 qubits was revealed~\cite{chow2021ibm}. However, the actual capability of Quantum Machine Learning (QML) is still unknown. Multiple reports~\cite{huang2022quantum}~\cite{schuld2022quantum} show that it indeed outperforms classical machine learning in certain situations. Most of the QML models are built on top of Variational Quantum Circuits (VQC), the quantum algorithms that depend on free parameters as known as Parametrized Quantum Circuits. Based on VQC, multiple quantum Feedforward neural networks capable of universal quantum computation have been proposed~\cite{schuld2014quest}~\cite{wan2017quantum}~\cite{farhi2018classification}~\cite{mitarai2018quantum}~\cite{beer2020training} with the goal of using the potential of quantum computers in machine learning and attempting to train networks more effectively.

% Introduce the work.
In this work, we propose QNet, a novel sequence encoder block that entirely inferences on quantum computer. The architecture of QNet is inspired by Transformer Encoder. In addition, the model composed of residual connected QNet blocks, ResQNet, is proposed. ResQNet has shown the state-of-the-art performance while reducing the inference time complexity on a variety of natural language processing tasks, including text classification, review score prediction, and cross word in natural language processing. 
For text classification, two datasets are used for the evaluation. On the Stackoverflow Question classification dataset, ResQNet achieves accuracy of 58.47\% with the embedding size of 2, while the best classical model with the same embedding size only reaches 7.15\%. On the ColBERT sentence emotion classification dataset, with only 1 embedding dimension, ResQNet can achieve accuracy of 91.58\%, while others need to have much larger embedding dimension to get their accuracy higher than 90\%. 
*** regression and nlp task results ***

% Show contribution details
The contributions of this research work are as follows.
\begin{itemize}
  \item We introduce a new model, QNet, a quantum native encoder block that uses the Quantum Fourier Transform and Variational Quantum Eigensolver to mix and transform input tokens. QNet has excellent analytical inference speed.
  \item To demonstrate the power of QNet on the quantum computer at the current stage, we propose ResQNet, a quantum-classical hybrid model that combines several QNet blocks by residual connections. With a much smaller embedding size, ResQNet is able to outperform the state-of-the-art models.
  \item We conduct extensive experiments to demonstrate the advantage of quantum computing in machine learning applications. Specifically, we found that quantum machine learning is superior than classical machine learning in a variety of natural language processing tasks.
\end{itemize}

% Show paper organization
The rest of this paper is organized as follows. Section~\ref{section:related} shows the existing works that are related to this research. Section~\ref{section:preliminary} brings in the required quantum computing knowledge. Section~\ref{section:method} describes the proposed model and the details of the work. Section~\ref{section:analysis} examines the time complexity of the model and compared with others. Section~\ref{section:experiment} verifies the practicability of the work. Section~\ref{section:conclusion} concludes this research.
