\section{Related Work}

\subsection{Quantum Neural Network}
Quantum neural networks have strong potential to be superior to the classical neural network after combining neural computing with the mechanics in quantum computing. Quantum data is in the form of quantum states. Just as a classical bit has a state 0 or 1, a qubit also has a state. Two possible state for a qubit are the states |0⟩ and |1⟩. The examples can be the two different polarizations of a photon and two states of an electron orbiting a single atom. Quantum neural networks can also process real-world data. These research achievements mainly include the following aspects: solving central tasks in quantum learning~\cite{}; enhancing the problem of machine learning~\cite{}; and efficient classification of quantum data~\cite{}. Among them, those papers about quantum learning for an unknown unitary transformation impress us a lot. In detail, Bisio and Chiribella~\cite{} addressed this task and found optimal strategy to store and retrieve an unknown unitary transformation on a quantum memory. Soon after, Sedla \etal~\cite{} designed an optimal protocol of unitary channels, which generalizes the results in~\cite{}. Moreover, Beer \etal~\cite{} proposed a quantum neural network with remarkable generalisation behaviour for the task of learning an unknown unitary quantum transformation. In this work, we are unable to employ the quantum neural network model due to the limited size of the experiment devices but theoretically our works can also adapt quantum neual network by replacing VQE circuit with quantum feedforward neural network.

\subsection{Quantum Recurrent Neural Network}



\subsection{Transformer Family}

With the development of deep learning, Natural Language Processing (NLP) technology has become progressively mature. In recent years, it has achieved very good results in tasks such as sentiment analysis~\cite{}, machine translation~\cite{}, speech recognition~\cite{}, and dialogue robots~\cite{}.
Natural language processing refers to a technology that enables computers to analyze and understand human language, and human language has a sequence and context relationship. Recurrent Neural Network (RNN) is often used for such time series data. However, because RNN is difficult to perform parallel operations, Google proposed a network architecture that does not use RNN and CNN, but only a self-attention mechanism - Transformer~\cite{NIPS2017_3f5ee243}, in which each block is composed of Multi-head attention and a Feedforward layer.

The Transformer architecture has inspired multiple large scale NLP models. OpenAI~\cite{}, an AI research institute, released a new generation of learning language model Generative Pretrained Transformer ver. 3 (GPT-3)~\cite{NEURIPS2020_1457c0d6}. GPT-3 is a general natural language processing model, through the neural network model can analyze the corpus, including strings, articles, etc., and expand on this basis to generate new texts. However, the computation cost of such large-scale model is miserable. GPT-3 could have easily cost 10 or 20 million dollars to train~\cite{}.

Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. BERT model is mainly compose with a stack of Transformer encoder and a classification layer. BERT is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context. The BERT framework was pre-trained using text from Wikipedia and can be fine-tuned with question and answer datasets.

Recent works have shown the potential mathematics transformations that can replace Multi-head attention. The approach to replace self-attention with a transpose operation and a Feedforward layer have been proved to be able to achieve convincing results. The work in~\cite{DBLP:journals/corr/abs-2105-02723} starts the question about the primitive property of attention mechanism. The work by *** \etal~\cite{DBLP:journals/corr/abs-2105-03824} shows that standard unparameterized Fourier transform can speed up Transformer encoder architectures, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \emph{mix} input tokens.
