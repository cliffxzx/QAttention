\begin{table*}[htb!]
    \centering
    \begin{tabular}{ |l|c|c|c|c|c|r|  }
        \hline
        Model & Epoch & Batch size & Embedding size & Num blocks & Initial LR & Accuracy \\
        \hline
        ResQNet & 5 & 128 & 1 & 1 & 0.0003 & \textbf{91.58} \\
        FNet & & & & & 1e-5 & 50.45 \\
        Transformer & & & & & & 50.45 \\
        \hline
        ResQNet & 5 & 128 & 2 & 1 & 0.0003 & \textbf{91.72} \\
        FNet & & & & & 1e-5 & 76.13 \\
        Transformer & & & & & & 86.59 \\
        \hline
        ResQNet & 5 & 128 & 3 & 1 & 0.0003 & \textbf{91.84} \\
        \hline
        FNet & 5 & 128 & 4 & 1 & 1e-5 & 90.83 \\
        Transformer & & & & & & \textbf{92.29} \\
        FNet & & & 8 & & & 91.99 \\
        \hline
    \end{tabular}
    \caption{The comparison of models when evaluating on ColBERT dataset.}
    \label{table:colbert_result}
\end{table*}

\begin{table*}[htb!]
    \centering
    \begin{tabular}{ |l|c|c|c|c|c|r|  }
        \hline
        Model & Epoch & Batch size & Embedding size & Num blocks & Initial LR & Accuracy \\
        \hline
        ResQNet & 5 & 128 & 1 & 1 & 0.0003 & \textbf{22.12}\\
        FNet & & & & & 1e-5 & 4.65\\
        Transformer & & & & & & 4.41\\
        \hline
        ResQNet & 5 & 128 & 2 & 1 & 0.0003 & \textbf{58.47} \\
        FNet & & & & & 1e-5 & 7.15 \\
        Transformer & & & & & & 5.56 \\
        Transformer & & & & 2 & & 6.35 \\
        Transformer & & & & 4 & 0.0003 & 6.53 \\
        \hline
        ResQNet & 5 & 128 & 3 & 1 & 0.0003 & \textbf{70.97} \\
        \hline
        FNet & 5 & 128 & 4 & 1 & 1e-5 & 24.00 \\
        Transformer & & & & & & 52.47 \\
        FNet & & & 8 & & & 48.68 \\
        Transformer & & & & & & \textbf{73.65} \\
        FNet & & & 16 & & & 71.18 \\
        \hline
    \end{tabular}
    \caption{The comparison of models when evaluating on Stackoverflow dataset.}
    \label{table:stackoverflow_result}
\end{table*}

\section{Experiments}

\subsection{Tasks}

In our experiments, we focus on natural language processing tasks that have short sequence input. The models are evaluated on classification task, regression task, and cross word task. All sentences are converted into lowercase and punctuation stripped. In addition, the consecutive tokens are separated by a white space. 

For text classification task, two datasets are used to evaluate the models, ColBERT~\cite{fw8e-z983-21} and StackOverflow~\cite{xu-etal-2015-short}. For review score prediction task, RentTheRunway~\cite{10.1145/3240323.3240398} dataset is used. For cross word task, Telomere-to-telomere~\cite{miga2020telomere} dataset is used. The descriptions of these datasets are as follows. % dataset and corresponding task

\subsubsection{Text Classification}
\begin{itemize}
  \item ColBERT dataset is a binary classification task for humor detection. It is consisted of 200,000 formal short texts (100,000 positive and 100,000 negative).
  \item StackOverflow dataset is a multi-class classification task to classify questions on StackOverflow.com. The data is consisted of randomly selected 20,000 question titles from 20 different tags (classes).
\end{itemize}

\subsubsection{Review Score Prediction}
RentTheRunway dataset is a regression task. The data are the measurements of clothing fit. We take review summary as input and predict the rating.

\subsubsection{Cross word}
Telomere-to-telomere dataset is human genome assembly. The input words are randomly masked, and the models have to predict the masked words.

The statistics of datasets are shown in Table~\ref{table:dataset-statistic}.

\begin{table}[htb!]
    \centering
    \begin{tabular}{ l|c|c|c|c  }
        \hline
        Dataset & Classes & Size & Length & $|V|$ \\
        \hline
        StackOverflow & 20 & 20,000 & 8.31/34 & 22,956 \\
        ColBERT & 2 & 200,000 & 12.81/22 & 74,010\\
        \hline
    \end{tabular}
    \caption{Statistics for the text datasets. Length (mean/max): the mean and max length of texts, and $|V|$: the vocabulary size.}
    \label{table:dataset-statistic}
\end{table}

\begin{table*}[htb!]
    \centering
    \begin{tabular}{ |l|c|c|c|c|c|r|  }
        \hline
        Model & Epoch & Batch size & Embedding size & Num blocks & Initial LR & Error \\
        \hline
        ResQNet & 5 & 128 & 1 & 1 & 0.0003 & \textbf{1.4865}\\
        FNet & & & & & 1e-5 & 2.0602 \\
        Transformer & & & & & & 2.0602 \\
        \hline
        ResQNet & 5 & 128 & 2 & 1 & 0.0003 & \textbf{1.4846} \\
        FNet & & & & & 1e-5 & 2.0654 \\
        Transformer & & & & & & 2.0603 \\
        % \hline
        % ResQNet & 5 & 128 & 3 & 1 & 0.0003 & \textbf{} \\
        \hline
        FNet & 5 & 128 & 4 & 1 & 1e-5 & 1.4516 \\
        Transformer & & & & & & \textbf{1.3773} \\
        \hline
    \end{tabular}
    \caption{The comparison of models when evaluating on RentTheRunway dataset.}
    \label{table:renttherunway}
\end{table*}

\subsection{Experimental Configuration}

This section describes the training regime to evaluate our models. We implemented the QNet with TensorFlow~\cite{abadi2016tensorflow}, a machine learning framework, and TensorFlow Quantum~\cite{broughton2020tensorflow}, a quantum machine learning library.
ResQNet is compared against Transformer and FNet under the common setting. Models are used as the backend for the same task. The learning rate is adjusted so that each model has the best validation accuracy.

\subsubsection{Hardware}

GPUs and TPUs are not used in the experiments due to the insufficient memory to simulate quantum computer. Instead, our model are trained distributed in supercomputer cluster, the Taiwania 1. In this work, 4 computing nodes are used, each node are equipped double 20 cores Intel Xeon Gold 6148 CPU and 384 GB RAM. To train the model in the cluster, the MultiWorkerMirroredStrategy of TensorFlow are used as the distribution strategy for synchronous training on multiple workers.

\subsubsection{Optimizer}

We used the Adam optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.98$ and $\epsilon = 10^{-9}$.

In the distributed training environment, the loss of model is reduced to share cross multiple workers. Therefore, we have to adjust the learning rate according to batch size and number of nodes. The adjusted learning rate is defined as global learning rate in Equation \ref{equ:global_lr}.

\begin{equation} \label{equ:global_lr}
global\_lr = initial\_lr \times batch\_size \times num\_nodes
\end{equation}

We varied the learning rate over the course of training, according to the Cosine Decay strategy in Equation \ref{equ:lr}. In this work, $\alpha = 10^{-2}$.

\begin{equation} \label{equ:lr}
lr = global\_lr \times ( \frac{1}{2} (1 + cos( \frac{step}{total\_steps}\pi))(1 - \alpha) + \alpha)
\end{equation}

\subsection{Results}

\subsubsection{Text Classification}
For the text classification tasks, cross entropy are used to minimize the distance to the target. In specific, Categorical Cross Entropy loss is used for multi-class classification and Binary Cross Entropy for binary classification.

The configurations and results of ColBERT humor detection task are listed in Table~\ref{table:colbert_result}. As can be seen in the table, ResQNet achieves accuracy of 91.58\% with only 1 embedding dimension. Other models can only reach 90\% with more than 4 embedding dimensions.

The configurations and results of StackOverflow question classification task are listed in Table~\ref{table:stackoverflow_result}. As can be seen in the table, ResQNet outperforms other models more than 50\% in terms of accuracy when the embedding size is 2. Transformer needs embedding size of 4 to reach 52.47\% and FNet needs 8 embedding dimensions to reach 48.68\%. We tested multiple blocks with Transformer on embedding dimensions of 2, and it shows that the gap between ResQNet and Transformer is hard to close up using deeper neural networks.

\subsubsection{Review Score Prediction}
The review score prediction tasks are been viewed as regression tasks, so mean square error is used to optimize the model and as the evaluation metric on test data.

The configurations and results of the RentTheRunway review rating prediction task are listed in Table~\ref{table:renttherunway}. As can be observed in the table, ResQNet performs 0.57 lower error than other models with the embedding size of 2. 
