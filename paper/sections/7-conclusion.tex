\section{Conclusion}

In this work, we presented the QNet, the first quantum-native sequence encoder model based on Transformer. And the ResQNet, the residual connected QNet to make use of QNet in the current stage of quantum computing resource.

For natural language processing tasks, the QNet can be outperformed significantly better than classical machine learning architectures on low embedding dimensions. On StackOverflow and ColBERT text classification task, we achieve the state of the art with all models under same setting.

We are excited about to train QNet in the large-scale NISQ quantum computer in the future. Therefore, We plan to extend the ResQNet to a pure quantum residual neural networks model. Design a efficient training routine for quantum-native models is another research goals of ours.
